<h4 id="background">Background:</h4>
<p>This paper banks on the concept of transformers and self attention both of which are foriegn to me. So i start out doing a brief review on what the two concepts are and then move onto what this paper does and how it uses them.</p>

<p>Review of the <a href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer Blog Post</a>. To start off with you need to first get an understanding of the attention mechanism in the Seq2Seq models starting <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">here</a></p>

<h4 id="key-ideas-from-the-two-blog-posts-on-attention-and-transformers">Key Ideas from the two blog posts on attention and transformers:</h4>

<h4 id="attention-mechanism">Attention Mechanism:</h4>
<ul>
  <li>
    <p>The attention mechanism is the same concept as the attention that we think of in computer vision as heatmaps. In there the common practise is to use a grad-cam to backpropogate and weight the regions of the image which correspond to that class output. The same concept applies here as well. You use the idea of weighting the hidden layers of the encoder for every step of the decoder to highlight the importance. This is used along with the hidden layer of the decoder to predict the output of every step of the decoder.</p>
  </li>
  <li>
    <p>One key difference in using attention networks versus classic seq-to-seq networks is the amount of information that gets passed onto the decoder. In classic case only the final Hidden State is passed to the decoder. Here all the hidden states are passed.</p>
  </li>
  <li>
    <p>At each time step of the decoder, each of the hidden layers from the encoder is then weighted according to the scoring function (normalized) to give higher importance or attention to the relavent hidden state.</p>
  </li>
  <li>
    <p>Further in transformers there is an attention mechanism in the encoder part as well</p>
  </li>
  <li>
    <p>Note that the scoring is calculated at each time step of the decoder.</p>
  </li>
</ul>

<p>This excellent illustration from Jay Alammar shows a gist of the seq2seq with attention. Plese refer to the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">original blogpost</a> for further details:</p>

<video width="100%" height="auto" loop="" autoplay="" controls="">
   <source src="/images/attention_tensor_dance.mp4" type="video/mp4" />
   Your browser does not support the video tag.
</video>

<h4 id="the-transformer">The transformer:</h4>

<ul>
  <li>
    <p>The transform is a different beast from what we have seen with the standard CNNs. It’s also based on the attention mechanism but in a highly structured way. Compared to the earlier attention mechanism in the seq2seq work with attention, this one has three different parts <strong>(Query, Key, Value)</strong> to the self-attention mechanism.</p>
  </li>
  <li>
    <p>The entire transformer network idea is to increase the number of self-attention heads and initialize them randomly so that you have enough different <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q, W_K, W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> matrices trained to ensure that the words are propoerly attended to.</p>
  </li>
  <li>
    <p>Further the decoder part of the network also has a similar attention mechanism to attend to the enconding output vectors. Unlike the original seq2seq with attention, here the network looks at the key and the value embeddings produced from the output of the final layer of the encoder. The weighting provided by the Query term of the decoder which only looks at the earlier positions of the output sequence is used to weight the attention for each decoder vector.</p>
  </li>
  <li>
    <p>Two additional conceps of <strong>residual connections</strong> and <strong>positional encodings</strong> for the input vector embeddings are used to further enchance the network. Residual connections function similar to the residual blocks that are used in Resnets and DenseNets of computer vision, i.e. they reduce the problem of vanishing gradient which will affect such huge networks like the transformers. The positional encoding is an intersting idea. They way I think of it is to liken it to a method which helps spatially encoder the positioning. The original paper broke the two halves of the 512 vector embedding into sine and cosine based embedding vectors and then concatenated them.</p>
  </li>
</ul>

<p>Additional Resource:
https://www.youtube.com/watch?v=rBCqOTEfxvg
http://nlp.seas.harvard.edu/2018/04/03/attention.html - A full step by step implementation of the paper with comments.</p>

<hr />

<h4 id="arxiv-paper-on-the-relationship-between-self-attention-and-convolutional-layers">Arxiv paper: <a href="https://arxiv.org/abs/1911.03584">ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS</a></h4>
<p>Blog post: http://jbcordonnier.com/posts/attention-cnn/</p>

<p>tl;dr: Self attention can express convolutional layer and the filters can be learnt in practise. The authors prove a theorem showing that with enough attention head, the self-attention can generalize the convolutional layer. I still haven’t read the paper in detail but their blog post gave me enough of an idea to understand what the paper is doing</p>

<h4 id="overall-impression">Overall impression</h4>
<p>Intersting take on correlating the attention head and cnns for images. The CNNs were originally designed to have limited to small fixed kernels which translated across the image to avoid having the whole image have differnt kernel weights. Now with the attention, it can attend to the whole image once again by posing it as an attention query problem.</p>

<h4 id="key-ideas">Key ideas</h4>
<ul>
  <li>Having multi head attention can generalize a CNN. Paper proves this theorem.</li>
  <li>With multiple heads ensures that all areas of the imgage are attended to.</li>
  <li>With positional encoding the problem of attention mechanism having equivariance is allieviated.</li>
  <li>Results on CFAIR show comparable performance to standard CNNs</li>
</ul>

<h4 id="technical-details">Technical details</h4>
<ul>
  <li>Attention mechanism translated from 1D to images: Basically instead of having token length embeddings for every word every single pixel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> is assigned a key - value score. There is a bit of notation abuse in the paper wherin every <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">A_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">X_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> denotes a 2D index corresponding to the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> pixel position.</li>
</ul>

<h4 id="notes">Notes</h4>
<ul>
  <li>Questions and notes on how to improve/revise the current work
    <ul>
      <li>Authors propose conditioning the receptive field on the input pixels</li>
      <li>Can this attention mechanism improve explainability. Can we better visualize what each region of the multiple heads are attending to for a given pixel or a region of pixels?</li>
      <li>Using transformers causes an explosion of paramters. And the main factor which is limiting it is the assumption that you take every pixel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> and attend to it. One simplification is to replicate what the embedding vector does in NLP. The same words would have the same embeddings. Maybe a simplification would be to have key-value pairs based on pixel intensity? Will the work out?</li>
    </ul>
  </li>
</ul>

<p>I skimmed the proof because I really don’t have any experience understanding what it is doing.</p>
