<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>On the Relationship between Self Attention and Convolutional Layers | Kashyap</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="On the Relationship between Self Attention and Convolutional Layers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers" />
<meta property="og:description" content="A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers" />
<link rel="canonical" href="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html" />
<meta property="og:url" content="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html" />
<meta property="og:site_name" content="Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-27T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers","@type":"BlogPosting","headline":"On the Relationship between Self Attention and Convolutional Layers","dateModified":"2020-01-27T00:00:00-06:00","url":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html","datePublished":"2020-01-27T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/learningblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hawkeyedesi.github.io/learningblog/feed.xml" title="Kashyap" /><link rel="shortcut icon" type="image/x-icon" href="/learningblog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>On the Relationship between Self Attention and Convolutional Layers | Kashyap</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="On the Relationship between Self Attention and Convolutional Layers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers" />
<meta property="og:description" content="A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers" />
<link rel="canonical" href="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html" />
<meta property="og:url" content="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html" />
<meta property="og:site_name" content="Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-27T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers","@type":"BlogPosting","headline":"On the Relationship between Self Attention and Convolutional Layers","dateModified":"2020-01-27T00:00:00-06:00","url":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html","datePublished":"2020-01-27T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://hawkeyedesi.github.io/learningblog/feed.xml" title="Kashyap" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/learningblog/">Kashyap</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/learningblog/about/">About Me</a><a class="page-link" href="/learningblog/search/">Search</a><a class="page-link" href="/learningblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">On the Relationship between Self Attention and Convolutional Layers</h1><p class="page-description">A review of the 2020 ICLR paper On the Relationship between Self Attention and Convolutional Layers</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-27T00:00:00-06:00" itemprop="datePublished">
        Jan 27, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/learningblog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h4"><a href="#background">Background:</a></li>
<li class="toc-entry toc-h4"><a href="#key-ideas-from-the-two-blog-posts-on-attention-and-transformers">Key Ideas from the two blog posts on attention and transformers:</a></li>
<li class="toc-entry toc-h4"><a href="#attention-mechanism">Attention Mechanism:</a></li>
<li class="toc-entry toc-h4"><a href="#the-transformer">The transformer:</a></li>
<li class="toc-entry toc-h4"><a href="#arxiv-paper-on-the-relationship-between-self-attention-and-convolutional-layers">Arxiv paper: ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS</a></li>
<li class="toc-entry toc-h4"><a href="#overall-impression">Overall impression</a></li>
<li class="toc-entry toc-h4"><a href="#key-ideas">Key ideas</a></li>
<li class="toc-entry toc-h4"><a href="#technical-details">Technical details</a></li>
<li class="toc-entry toc-h4"><a href="#notes">Notes</a></li>
</ul><h4 id="background">
<a class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background:</h4>
<p>This paper banks on the concept of transformers and self attention both of which are foriegn to me. So i start out doing a brief review on what the two concepts are and then move onto what this paper does and how it uses them.</p>

<p>Review of the <a href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer Blog Post</a>. To start off with you need to first get an understanding of the attention mechanism in the Seq2Seq models starting <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">here</a></p>

<h4 id="key-ideas-from-the-two-blog-posts-on-attention-and-transformers">
<a class="anchor" href="#key-ideas-from-the-two-blog-posts-on-attention-and-transformers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key Ideas from the two blog posts on attention and transformers:</h4>

<h4 id="attention-mechanism">
<a class="anchor" href="#attention-mechanism" aria-hidden="true"><span class="octicon octicon-link"></span></a>Attention Mechanism:</h4>
<ul>
  <li>
    <p>The attention mechanism is the same concept as the attention that we think of in computer vision as heatmaps. In there the common practise is to use a grad-cam to backpropogate and weight the regions of the image which correspond to that class output. The same concept applies here as well. You use the idea of weighting the hidden layers of the encoder for every step of the decoder to highlight the importance. This is used along with the hidden layer of the decoder to predict the output of every step of the decoder.</p>
  </li>
  <li>
    <p>One key difference in using attention networks versus classic seq-to-seq networks is the amount of information that gets passed onto the decoder. In classic case only the final Hidden State is passed to the decoder. Here all the hidden states are passed.</p>
  </li>
  <li>
    <p>At each time step of the decoder, each of the hidden layers from the encoder is then weighted according to the scoring function (normalized) to give higher importance or attention to the relavent hidden state.</p>
  </li>
  <li>
    <p>Further in transformers there is an attention mechanism in the encoder part as well</p>
  </li>
  <li>
    <p>Note that the scoring is calculated at each time step of the decoder.</p>
  </li>
</ul>

<p>This excellent illustration from Jay Alammar shows a gist of the seq2seq with attention. Plese refer to the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">original blogpost</a> for further details:</p>

<video width="100%" height="auto" loop="" autoplay="" controls="">
   <source src="/images/attention_tensor_dance.mp4" type="video/mp4"></source>
   Your browser does not support the video tag.
</video>

<h4 id="the-transformer">
<a class="anchor" href="#the-transformer" aria-hidden="true"><span class="octicon octicon-link"></span></a>The transformer:</h4>

<ul>
  <li>
    <p>The transform is a different beast from what we have seen with the standard CNNs. Itâ€™s also based on the attention mechanism but in a highly structured way. Compared to the earlier attention mechanism in the seq2seq work with attention, this one has three different parts <strong>(Query, Key, Value)</strong> to the self-attention mechanism.</p>
  </li>
  <li>
    <p>The entire transformer network idea is to increase the number of self-attention heads and initialize them randomly so that you have enough different <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q, W_K, W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> matrices trained to ensure that the words are propoerly attended to.</p>
  </li>
  <li>
    <p>Further the decoder part of the network also has a similar attention mechanism to attend to the enconding output vectors. Unlike the original seq2seq with attention, here the network looks at the key and the value embeddings produced from the output of the final layer of the encoder. The weighting provided by the Query term of the decoder which only looks at the earlier positions of the output sequence is used to weight the attention for each decoder vector.</p>
  </li>
  <li>
    <p>Two additional conceps of <strong>residual connections</strong> and <strong>positional encodings</strong> for the input vector embeddings are used to further enchance the network. Residual connections function similar to the residual blocks that are used in Resnets and DenseNets of computer vision, i.e. they reduce the problem of vanishing gradient which will affect such huge networks like the transformers. The positional encoding is an intersting idea. They way I think of it is to liken it to a method which helps spatially encoder the positioning. The original paper broke the two halves of the 512 vector embedding into sine and cosine based embedding vectors and then concatenated them.</p>
  </li>
</ul>

<p>Additional Resource:
https://www.youtube.com/watch?v=rBCqOTEfxvg
http://nlp.seas.harvard.edu/2018/04/03/attention.html - A full step by step implementation of the paper with comments.</p>

<hr>

<h4 id="arxiv-paper-on-the-relationship-between-self-attention-and-convolutional-layers">
<a class="anchor" href="#arxiv-paper-on-the-relationship-between-self-attention-and-convolutional-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Arxiv paper: <a href="https://arxiv.org/abs/1911.03584">ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS</a>
</h4>
<p>Blog post: http://jbcordonnier.com/posts/attention-cnn/</p>

<p>tl;dr: Self attention can express convolutional layer and the filters can be learnt in practise. The authors prove a theorem showing that with enough attention head, the self-attention can generalize the convolutional layer. I still havenâ€™t read the paper in detail but their blog post gave me enough of an idea to understand what the paper is doing</p>

<h4 id="overall-impression">
<a class="anchor" href="#overall-impression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overall impression</h4>
<p>Intersting take on correlating the attention head and cnns for images. The CNNs were originally designed to have limited to small fixed kernels which translated across the image to avoid having the whole image have differnt kernel weights. Now with the attention, it can attend to the whole image once again by posing it as an attention query problem.</p>

<h4 id="key-ideas">
<a class="anchor" href="#key-ideas" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key ideas</h4>
<ul>
  <li>Having multi head attention can generalize a CNN. Paper proves this theorem.</li>
  <li>With multiple heads ensures that all areas of the imgage are attended to.</li>
  <li>With positional encoding the problem of attention mechanism having equivariance is allieviated.</li>
  <li>Results on CFAIR show comparable performance to standard CNNs</li>
</ul>

<h4 id="technical-details">
<a class="anchor" href="#technical-details" aria-hidden="true"><span class="octicon octicon-link"></span></a>Technical details</h4>
<ul>
  <li>Attention mechanism translated from 1D to images: Basically instead of having token length embeddings for every word every single pixel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> is assigned a key - value score. There is a bit of notation abuse in the paper wherin every <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">A_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">X_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> denotes a 2D index corresponding to the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> pixel position.</li>
</ul>

<h4 id="notes">
<a class="anchor" href="#notes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notes</h4>
<ul>
  <li>Questions and notes on how to improve/revise the current work
    <ul>
      <li>Authors propose conditioning the receptive field on the input pixels</li>
      <li>Can this attention mechanism improve explainability. Can we better visualize what each region of the multiple heads are attending to for a given pixel or a region of pixels?</li>
      <li>Using transformers causes an explosion of paramters. And the main factor which is limiting it is the assumption that you take every pixel <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i,j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span></span></span></span> and attend to it. One simplification is to replicate what the embedding vector does in NLP. The same words would have the same embeddings. Maybe a simplification would be to have key-value pairs based on pixel intensity? Will the work out?</li>
    </ul>
  </li>
</ul>

<p>I skimmed the proof because I really donâ€™t have any experience understanding what it is doing.</p>

  </div><a class="u-url" href="/learningblog/markdown/2020/01/27/self_attention_iclr2020.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/learningblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/learningblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/learningblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blogging about Deep Learning, Medical Imaging and Vision Applications</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/hawkeyedesi" title="hawkeyedesi"><svg class="svg-icon grey"><use xlink:href="/learningblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/follow_kashyap" title="follow_kashyap"><svg class="svg-icon grey"><use xlink:href="/learningblog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
