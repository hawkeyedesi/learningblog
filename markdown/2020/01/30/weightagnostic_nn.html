<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Weight Agnostic Neural Networks | Kashyap</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Weight Agnostic Neural Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A review of the Weight Agnostic Neural Networks paper." />
<meta property="og:description" content="A review of the Weight Agnostic Neural Networks paper." />
<link rel="canonical" href="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html" />
<meta property="og:url" content="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html" />
<meta property="og:site_name" content="Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-30T00:00:00-06:00" />
<script type="application/ld+json">
{"dateModified":"2020-01-30T00:00:00-06:00","datePublished":"2020-01-30T00:00:00-06:00","description":"A review of the Weight Agnostic Neural Networks paper.","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html"},"url":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html","headline":"Weight Agnostic Neural Networks","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/learningblog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hawkeyedesi.github.io/learningblog/feed.xml" title="Kashyap" /><link rel="shortcut icon" type="image/x-icon" href="/learningblog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Weight Agnostic Neural Networks | Kashyap</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Weight Agnostic Neural Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A review of the Weight Agnostic Neural Networks paper." />
<meta property="og:description" content="A review of the Weight Agnostic Neural Networks paper." />
<link rel="canonical" href="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html" />
<meta property="og:url" content="https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html" />
<meta property="og:site_name" content="Kashyap" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-30T00:00:00-06:00" />
<script type="application/ld+json">
{"dateModified":"2020-01-30T00:00:00-06:00","datePublished":"2020-01-30T00:00:00-06:00","description":"A review of the Weight Agnostic Neural Networks paper.","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html"},"url":"https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html","headline":"Weight Agnostic Neural Networks","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://hawkeyedesi.github.io/learningblog/feed.xml" title="Kashyap" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/learningblog/">Kashyap</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/learningblog/about/">About Me</a><a class="page-link" href="/learningblog/search/">Search</a><a class="page-link" href="/learningblog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Weight Agnostic Neural Networks</h1><p class="page-description">A review of the Weight Agnostic Neural Networks paper.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-30T00:00:00-06:00" itemprop="datePublished">
        Jan 30, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/learningblog/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h4"><a href="#resources">Resources</a></li>
<li class="toc-entry toc-h4"><a href="#overall-impression">Overall impression</a></li>
<li class="toc-entry toc-h4"><a href="#key-ideas">Key ideas</a></li>
<li class="toc-entry toc-h4"><a href="#technical-details">Technical details</a></li>
<li class="toc-entry toc-h4"><a href="#notes">Notes</a></li>
<li class="toc-entry toc-h4"><a href="#things-i-dont-understand">Things I don’t understand </a></li>
</ul><h4 id="resources">
<a class="anchor" href="#resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources</h4>
<ul>
  <li><strong><a href="https://arxiv.org/abs/1906.04358">Arxiv Paper</a></strong></li>
  <li><strong><a href="https://weightagnostic.github.io/">Blogpost</a></strong></li>
  <li><strong><a href="https://weightagnostic.github.io/slides/wann_slides.pdf">Slides</a></strong></li>
  <li><strong><a href="https://github.com/weightagnostic/weightagnostic.github.io/issues">Blog Github</a></strong></li>
  <li><strong><a href="https://arxiv.org/pdf/math/0406077.pdf">Tutorial on MDL</a></strong></li>
</ul>

<p><strong>Note: I am basing everything in these notes from the excellent blog post. I haven’t gone through the paper yet.</strong></p>

<p>tl;dr: The paper introduces a method of designing neural network architectures that are not dependent on the node weights. Rather the trick here is to let the network evolve its architecture such that the architecture itself has the capacity required to solve the required tasks. The authors draw analogies with precocial animals such as snakes and iguanas wherein they are wired to run away from the prey.</p>

<h4 id="overall-impression">
<a class="anchor" href="#overall-impression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Overall impression</h4>
<p>I think its a very cool concept in its nascent stages they have shown how without the burden of weight training, they employed a topology-based algorithm (<a href="http://www.cs.ucf.edu/~kstanley/neat.html">NEAT</a>) to find and grow the topology.</p>

<h4 id="key-ideas">
<a class="anchor" href="#key-ideas" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key ideas</h4>
<p>Quoting from the blog:</p>
<blockquote>
  <p>While the aforementioned works focus on the information capacity required to represent the weights of predefined network architecture, in this work we focus on finding minimal architectures that can represent solutions to various tasks.</p>
</blockquote>

<ul>
  <li>
    <p>By making the network be independent of the weights or the tuning and solely depending on the architecture the work can use other algorithms besides gradient descent and backprop such as NEAT to solve the architecture</p>
  </li>
  <li>
    <p>By weight sharing across the entire network with the weighting treated as a random sample from a uniform random distribution.</p>
  </li>
  <li>
    <p>Introducing more than the common (e.g. linear, sigmoid, ReLU) and more exotic (Gaussian, sinusoid, step), the network is learning to include more of these connections.</p>
  </li>
  <li>
    <p>By replacing the grunt work of the weights as filters, this work is now forcing the network connections to provide the right weighting or inductive biases for that particular problem.</p>
  </li>
  <li>
    <p>Further, instead of fixing the structure of the network and then biasing the weights of the network to solve a specific problem, you are instead relying on the network architecture structure.</p>
  </li>
  <li>
    <p>By introducing nodes with a lot more sets of options such as inv, sin, cost gaussian, bias, etc (See paper and the code for details) you are replicating the functions a weight-based filter would do. That is a pretty cool concept.</p>
  </li>
  <li>
    <p>Unlike NAS or other operations wherein you have to run the network every single time to compute a metric (for eg. accuracy) to see the performance, this proposed method instead just goes with random permutations of the network (I am assuming that’s what the random trial paper does or some flavor of it).</p>
  </li>
</ul>

<h4 id="technical-details">
<a class="anchor" href="#technical-details" aria-hidden="true"><span class="octicon octicon-link"></span></a>Technical details</h4>
<ul>
  <li>
    <p>Not merely any weight agnostic neural network but one that can be described using a minimum descriptor length.</p>
  </li>
  <li>
    <p>Significance of minimum description length models when choosing the weight agnostic NN: Using MDL inference, you are ensuring that the model that is being selected is the simplest one while still ensuring that it works just as well as the more complex network architectures.</p>
  </li>
</ul>

<h4 id="notes">
<a class="anchor" href="#notes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notes</h4>
<ul>
  <li>
    <p>Inductive Biases in Machine learning are a set of assumptions that the model learns to make during training to predict the unseen target data accurately. This <a href="http://www.lauradhamilton.com/inductive-biases-various-machine-learning-algorithms">website</a> on inductive bias has a nice list of biases that are present for every task.</p>
  </li>
  <li>
    <p>What is Minimum Description Length: MDL is an inference method that provides a generic solution to the model inference problem. The definition states that when you have different models that perform just as well, using MDL for model selection will ensure that you pick the simplest model that still performs as well as the more complex models.</p>
  </li>
  <li>Questions and notes on how to improve/revise the current work
    <ul>
      <li>How would it work for more complicated semi-supervised and supervised problems?</li>
      <li>Are there more creative and interesting node functions to pick from that will help improve this?</li>
      <li>What happens if you start allowing more interesting connections such as skip connections in the feed-forward directions. Can you even do that and can it be optimized using the NEAT algorithm? 
 
        <h4 id="things-i-dont-understand">
<a class="anchor" href="#things-i-dont-understand" aria-hidden="true"><span class="octicon octicon-link"></span></a>Things I don’t understand </h4>
      </li>
    </ul>
  </li>
  <li>
    <p>What are the connection cost technique and I am not sure how it relates to the NEAT algorithm for model growing?</p>
  </li>
  <li>
    <p>Further, how does dominance relations work to rank the networks based on mean overall performance, the weight values (how does it perform for the fixed selected weight. The paper chose a range between [-2,2]), max performance of the single best value and the number of connections in the network.</p>
  </li>
  <li>Does the domain relation ranking also consider the MDL inference internally and if so how? I still need to read those topics.</li>
</ul>


  </div><a class="u-url" href="/learningblog/markdown/2020/01/30/weightagnostic_nn.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/learningblog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/learningblog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/learningblog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blogging about Deep Learning, Medical Imaging and Vision Applications</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/hawkeyedesi" title="hawkeyedesi"><svg class="svg-icon grey"><use xlink:href="/learningblog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/follow_kashyap" title="follow_kashyap"><svg class="svg-icon grey"><use xlink:href="/learningblog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
