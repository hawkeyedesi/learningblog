{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hawkeyedesi.github.io/learningblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "A Briefy Survey of the Latest in Unsupervised Domain adapatation . Detialed notes on some of the papers survey can be found here . Most commonly cited papers in the literature: . [1702.05464] Adversarial Discriminative Domain Adaptation . [1607.03516] Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation . [1809.02176] Multi-Adversarial Domain Adaptation . Latest Papers from AAAI Conference: . Multi-source Distilling Domain Adaptation . Domain Generalization Using a Mixture of Multiple Latent Domains . Adversarial Domain Adaptation with Domain Mixup . [2001.01046] Adversarial-Learned Loss for Domain Adaptation . Other Papers: . Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach . [1801.07593] Mitigating Unwanted Biases with Adversarial Learning This paper attempts to define the different types of biases that can occur and then tries to reduce it using adverserial techinques. The work is shown on both text and tabular data. Results show good outputs without dependence on the bias variable. . High-performance medicine: the convergence of human and artificial intelligence | Nature Medicine . A nice review paper on the overall state of ai in medicine. Nothing in particular related to domain adaptation mentioned although it is a review paper I am sure one of teh many papers cited would have done some bit of it. . International evaluation of an AI system for breast cancer screening | Nature . [1809.07294] Generative Adversarial Network in Medical Imaging: A Review A nice review of gans again not really specifically on domain adapatation. . IJCAI 2018: Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss . Idea is to do a cross domain segmentation using ct and mris. The network is posed as a domain adapatation problem with a adverserarial part. Idea here is to reduce the variances in the feature space between the source and the target dataset since the final task is the same, i.e. whole heart segemntation. This is acheived by having a seperate module trained for the target feature generator part (they call it DAM (domain adapatation module)) alond with a domain critic module (DCM) which uses the wasserstein distance to quanitfy the feature space differences between the MRI and the CT. . only feature space alignment with a few layers. only works on the same modality of the body parts does not or will not probabaly work if the source and the target datasets are acquired differently. .",
            "url": "https://hawkeyedesi.github.io/learningblog/markdown/2020/02/12/unsupervised_domain_adaptation.html",
            "relUrl": "/markdown/2020/02/12/unsupervised_domain_adaptation.html",
            "date": " • Feb 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "[REVIEW] Weight Agnostic Neural Networks . Resources . Arxiv Paper | Blogpost | Slides | Blog Github | Tutorial on MDL | . Note: I am basing everything in these notes from the excellent blog post. I haven’t gone through the paper yet. . tl;dr: The paper introduces a method of designing neural network architectures that are not dependent on the node weights. Rather the trick here is to let the network evolve its architecture such that the architecture itself has the capacity required to solve the required tasks. The authors draw analogies with precocial animals such as snakes and iguanas wherein they are wired to run away from the prey. . Overall impression . I think its a very cool concept in its nascent stages they have shown how without the burden of weight training, they employed a topology-based algorithm (NEAT) to find and grow the topology. . Key ideas . Quoting from the blog: . While the aforementioned works focus on the information capacity required to represent the weights of predefined network architecture, in this work we focus on finding minimal architectures that can represent solutions to various tasks. . By making the network be independent of the weights or the tuning and solely depending on the architecture the work can use other algorithms besides gradient descent and backprop such as NEAT to solve the architecture . | By weight sharing across the entire network with the weighting treated as a random sample from a uniform random distribution. . | Introducing more than the common (e.g. linear, sigmoid, ReLU) and more exotic (Gaussian, sinusoid, step), the network is learning to include more of these connections. . | By replacing the grunt work of the weights as filters, this work is now forcing the network connections to provide the right weighting or inductive biases for that particular problem. . | Further, instead of fixing the structure of the network and then biasing the weights of the network to solve a specific problem, you are instead relying on the network architecture structure. . | By introducing nodes with a lot more sets of options such as inv, sin, cost gaussian, bias, etc (See paper and the code for details) you are replicating the functions a weight-based filter would do. That is a pretty cool concept. . | Unlike NAS or other operations wherein you have to run the network every single time to compute a metric (for eg. accuracy) to see the performance, this proposed method instead just goes with random permutations of the network (I am assuming that’s what the random trial paper does or some flavor of it). . | . Technical details . Not merely any weight agnostic neural network but one that can be described using a minimum descriptor length. . | Significance of minimum description length models when choosing the weight agnostic NN: Using MDL inference, you are ensuring that the model that is being selected is the simplest one while still ensuring that it works just as well as the more complex network architectures. . | . Notes . Inductive Biases in Machine learning are a set of assumptions that the model learns to make during training to predict the unseen target data accurately. This website on inductive bias has a nice list of biases that are present for every task. . | What is Minimum Description Length: MDL is an inference method that provides a generic solution to the model inference problem. The definition states that when you have different models that perform just as well, using MDL for model selection will ensure that you pick the simplest model that still performs as well as the more complex models. . | Questions and notes on how to improve/revise the current work How would it work for more complicated semi-supervised and supervised problems? | Are there more creative and interesting node functions to pick from that will help improve this? | What happens if you start allowing more interesting connections such as skip connections in the feed-forward directions. Can you even do that and can it be optimized using the NEAT algorithm?   Things I don’t understand  . | . | What are the connection cost technique and I am not sure how it relates to the NEAT algorithm for model growing? . | Further, how does dominance relations work to rank the networks based on mean overall performance, the weight values (how does it perform for the fixed selected weight. The paper chose a range between [-2,2]), max performance of the single best value and the number of connections in the network. . | Does the domain relation ranking also consider the MDL inference internally and if so how? I still need to read those topics. | .",
            "url": "https://hawkeyedesi.github.io/learningblog/markdown/2020/01/30/weightagnostic_nn.html",
            "relUrl": "/markdown/2020/01/30/weightagnostic_nn.html",
            "date": " • Jan 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "[REVIEW] On the Relationship between Self Attention and Convolutional Layers . Background: . This paper banks on the concept of transformers and self attention both of which are foriegn to me. So i start out doing a brief review on what the two concepts are and then move onto what this paper does and how it uses them. . Review of the Illustrated Transformer Blog Post. To start off with you need to first get an understanding of the attention mechanism in the Seq2Seq models starting here . Key Ideas from the two blog posts on attention and transformers: . Attention Mechanism: . The attention mechanism is the same concept as the attention that we think of in computer vision as heatmaps. In there the common practise is to use a grad-cam to backpropogate and weight the regions of the image which correspond to that class output. The same concept applies here as well. You use the idea of weighting the hidden layers of the encoder for every step of the decoder to highlight the importance. This is used along with the hidden layer of the decoder to predict the output of every step of the decoder. . | One key difference in using attention networks versus classic seq-to-seq networks is the amount of information that gets passed onto the decoder. In classic case only the final Hidden State is passed to the decoder. Here all the hidden states are passed. . | At each time step of the decoder, each of the hidden layers from the encoder is then weighted according to the scoring function (normalized) to give higher importance or attention to the relavent hidden state. . | Further in transformers there is an attention mechanism in the encoder part as well . | Note that the scoring is calculated at each time step of the decoder. . | . This excellent illustration from Jay Alammar shows a gist of the seq2seq with attention. Plese refer to the original blogpost for further details: . Your browser does not support the video tag. The transformer: . The transform is a different beast from what we have seen with the standard CNNs. It’s also based on the attention mechanism but in a highly structured way. Compared to the earlier attention mechanism in the seq2seq work with attention, this one has three different parts (Query, Key, Value) to the self-attention mechanism. . | The entire transformer network idea is to increase the number of self-attention heads and initialize them randomly so that you have enough different WQ,WK,WVW_Q, W_K, W_VWQ​,WK​,WV​ matrices trained to ensure that the words are propoerly attended to. . | Further the decoder part of the network also has a similar attention mechanism to attend to the enconding output vectors. Unlike the original seq2seq with attention, here the network looks at the key and the value embeddings produced from the output of the final layer of the encoder. The weighting provided by the Query term of the decoder which only looks at the earlier positions of the output sequence is used to weight the attention for each decoder vector. . | Two additional conceps of residual connections and positional encodings for the input vector embeddings are used to further enchance the network. Residual connections function similar to the residual blocks that are used in Resnets and DenseNets of computer vision, i.e. they reduce the problem of vanishing gradient which will affect such huge networks like the transformers. The positional encoding is an intersting idea. They way I think of it is to liken it to a method which helps spatially encoder the positioning. The original paper broke the two halves of the 512 vector embedding into sine and cosine based embedding vectors and then concatenated them. . | . Additional Resource: https://www.youtube.com/watch?v=rBCqOTEfxvg http://nlp.seas.harvard.edu/2018/04/03/attention.html - A full step by step implementation of the paper with comments. . . Arxiv paper: ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS . Blog post: http://jbcordonnier.com/posts/attention-cnn/ . tl;dr: Self attention can express convolutional layer and the filters can be learnt in practise. The authors prove a theorem showing that with enough attention head, the self-attention can generalize the convolutional layer. I still haven’t read the paper in detail but their blog post gave me enough of an idea to understand what the paper is doing . Overall impression . Intersting take on correlating the attention head and cnns for images. The CNNs were originally designed to have limited to small fixed kernels which translated across the image to avoid having the whole image have differnt kernel weights. Now with the attention, it can attend to the whole image once again by posing it as an attention query problem. . Key ideas . Having multi head attention can generalize a CNN. Paper proves this theorem. | With multiple heads ensures that all areas of the imgage are attended to. | With positional encoding the problem of attention mechanism having equivariance is allieviated. | Results on CFAIR show comparable performance to standard CNNs | . Technical details . Attention mechanism translated from 1D to images: Basically instead of having token length embeddings for every word every single pixel i,ji,ji,j is assigned a key - value score. There is a bit of notation abuse in the paper wherin every ApA_pAp​ and XpX_pXp​ denotes a 2D index corresponding to the (x,y)(x,y)(x,y) pixel position. | . Notes . Questions and notes on how to improve/revise the current work Authors propose conditioning the receptive field on the input pixels | Can this attention mechanism improve explainability. Can we better visualize what each region of the multiple heads are attending to for a given pixel or a region of pixels? | Using transformers causes an explosion of paramters. And the main factor which is limiting it is the assumption that you take every pixel i,ji,ji,j and attend to it. One simplification is to replicate what the embedding vector does in NLP. The same words would have the same embeddings. Maybe a simplification would be to have key-value pairs based on pixel intensity? Will the work out? | . | . I skimmed the proof because I really don’t have any experience understanding what it is doing. .",
            "url": "https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/self_attention_iclr2020.html",
            "relUrl": "/markdown/2020/01/27/self_attention_iclr2020.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "[REVIEW] Differentiation of Blackbox Combinatorial Solvers . Arxiv Paper: Differentiation of Blackbox Combinatorial Solvers . tl;dr: This paper presents for the first time a method for combining deep learning as a rich feature generator along with a combinatorial optimizer. The innovative thing they show here is that by approximating the piecewise continuous outputs from a combinatorial optimizer to make it continuous we can backpropagate the gradients to also learn the feature representations of the deep learner.   Also disclaimer: This is all that I understood from the blog post and haven’t read the paper in detail. . Overall impression . I think it is a cool idea that you can present to the combinatorial optimization a set of learnable features for the problem using such a rich feature representer like deep learning while also solving complex combinatorial optimization. . Key ideas . Realize that by posing the combinatorial optimizer as one of the intermediate layers of the deep learner you can solve the problem along with optimizing the weights of the deep learner to improve the feature representation. | By ensuring that the output of the combinatorial optimizer can be made continuous by maintaining the same minima and maxima, you can end up passing the gradients through the combinatorial optimizer to update the weights of the neural network. | Different from the other papers is the fact that does a smarter piecewise approximation.   | They take existing combinatorial solvers and use it as a black box “Layer” for deep learning. | . Technical details . I don’t fully understand it. | . Notes . Questions and notes on how to improve/revise the current work   . | How this work would apply to improve your current research or new research directions   - What would happen if you had another 1D deep learning interpolator to improve the approximation of the piece-wise continuous function? How much dependent is the learner as a function of the optimizer? . | . Extra resources: . https://towardsdatascience.com/the-fusion-of-deep-learning-and-combinatorics-4d0112a74fa7 | Extra points to note | .",
            "url": "https://hawkeyedesi.github.io/learningblog/markdown/2020/01/27/blackbox_combinatorial_solver_iclr2020.html",
            "relUrl": "/markdown/2020/01/27/blackbox_combinatorial_solver_iclr2020.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hawkeyedesi.github.io/learningblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hawkeyedesi.github.io/learningblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}