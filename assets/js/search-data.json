{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hawkeyedesi.github.io/learningblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Multisource_distilling_da",
            "content": "Multi-source Distilling Domain Adaptation . (https://arxiv.org/abs/1911.11554) . Overall impression . The key concept in this work is a method of trying to uniquely maintain the individual source representations while at the same time fine-tuning the target to one of the sources based on a metric and choosing a weighting automatically to pick those sources that best represent a particular target. . The advantages the authors claim is that they are not rounding off the features of a particular learner and making it more and more homogeneous just to make it adapt to one another. . Instead, they are all learned uniquely and then the target feature extractor is fine-tuned to make sure that you can figure out if they can match the closest of the source target that represents it. . This method seems to show SOTA on some of the benchmark datasets for unsupervised domain adaptation. . Key ideas . Previous works treat the single source and single target unsupervised domain adaptation which is not the case in real-life scenarios. | Several of multiple source DA work have common issues: a. Treating all the sources equally, b. Eliminating the discriminative properties of the features to learn the domain invariant features, c. Treating each sample from the same source equally and not acknowledging the fact that the same source dataset also can have differences w.r.t to the targets, d. vanishing gradient problem of the discriminator when the target can perfectly distinguish between the source and the target feature representations. | Fix for each of the issues in the proposed work: Eliminating the discriminative properties of the features to learn the domain invariant features: Each source set is individually trained to maintain the unique set of features and the feature maps are then frozen to prevent fine-tuning by the discriminator during adversarial training. | Vanishing gradient problem of the discriminator when the target can perfectly distinguish between the source and the target feature representations: Each source is fixed and then adversarially map the target feature against each of the source datasets using the Wasserstein distance metric. This prevents vanishing gradients even when the source and the target are non-overlapping. | Treating each sample from the same source equally and not acknowledging the fact that the same source dataset also can have differences w.r.t to the targets: The targets are all fine-tuned with the source datasets that are closest to the target. | Treating all the sources equally: Weighting the output of each of the sources to ensure that the ones closest to the sources are chosen. | . | . Technical details . Each source is first fine-tuned for classification. The feature generator is then fixed. | The target feature generator with each module is trained separately along with the discriminator which maximizes the Wasserstein distance. Not only does this help align the feature generators of the target domain to each source, but the min-max two-player game also optimizes to maximize the Wasserstein distance which will later be used as a weighting function for the outputs of the feature generators on the test set. | Fine-tuning is performed based on the discriminator output and the classifier is fine-tuned with these source images to better align with the target domain. | . Notes . How does doing every discriminator with the individual target help? I think it’s inefficient. Wouldn’t doing it together actually improve the outcomes better. | Yes, they make a point of the domain generalization removing the discriminative features but then you can also probably still have a smarter sort of domain aggregation for each of the targets because this just feels very similar to the individual source to domain matching. The only thing that they are doing differently is using the Wasserstein distance in the discriminator to prevent mode collapse and then weighting and fine-tuning the individual source domain classifiers to make them work better with the target. -The authors claim that by individually training and then fine-tuning for the target leads to more optimal solutions. But then I am not clear on what would happen if you don’t have a particular source that is nowhere close to the set of datasets shown. What kind of weighting would help there? | . . Domain Generalization Using a Mixture of Multiple Latent Domains . (https://arxiv.org/abs/1911.07661) . Overall impression . This paper’s premise is that you sometimes don’t have the domain labels since the images are sometimes web-crawled. This paper proposes how to deal with it and claims that it beats the state of the art in the field as well including beating those that have the domain labels as well. . To do so they do an iterative clustering of the feature space and then slowly aggregate similar domains together assuming the features represent the domain that you are interested in the right way. . They assume that the latent domain in the image is represented by the style therefore it uses the style features for clustering since that better represents the features and the latent domains in the different sources. . Overall the key ideas are the use of style transfer to represent the feature clustering, a pseudo domain labelling using a k-means clustering, adverserial learning along with an entropy term to improve the overall domain generalization for the problem. . Key ideas . The domain during training does not always stay the same. For eg. When training on web-crawled data the domain within the training set itself varies. | The work hypothesis that there is a mixture of multiple latent domains and that domain generalization can be achieved by a combination of these latent domains. | To acheive this, the authors propose to automatically cluster the latent domains in the training set and the domain invariant features which are common amongst the different latent domain sets. | They hypothesize that the style features from the hidden layers of the neural network best represents the hidden latent domains and clustering on these style features would help best represet the different latent domains. | . Technical details . Style transfer originally used the gram matrices of the neural activation on the different convolutional layers of the neural network to show the style of the image. | It has been shown theoertically that the gram matrices of the convolutional layer activations are equivalent to minimizing the maximum mean descripancy with a second order polynomial by constructing another style loss image matching the convolutional feature statistics(mean and standard deviation) between the style image and the generated image. | The authors claim that you are trying to extract domain invariant features by making the outputs domain invariant and doing that will actually diminsh the domain discriminative features. And using style transfer should be able to help distinguish better. | Clustering is done use a k-means clustering algorithm. Since they are pseudo labels and might change every single iteration, a Kuhn-munkres algorithm is used to maximize the rate of cluster assignments and the pseudo domain labels | No significant(no t-test mind you) correlation between the number of pseudo labels assigned and the classification accuracy. (I assume it is to some extent true and if you dice it more and more it will affect the results). | Ablation studies show that pseudo domain labels match the original domain labels and they become stable as the training progresses. | Github code: https://github.com/mil-tokyo/dg_mmld/ | . Notes . The literature survey shows several intersting past works which they claim all require domain labels and hence they are different. there are works in a. Using auto encoder architecture, b. adverserial learning, c. Meta learning methods (MetaReg: Towards Domain Generalization using Meta-Regularization), d. Domain aggregator methods (which is similar to their clustering idea) and e. Using self-supervised and semi supervisied learning (Domain Generalization by Solving Jigsaw Puzzles) (which also does not require domain labels but assumes that the domains of the different labels are not mixed up and stay the same).   | Entropy loss so that at the decision boundary of the adverserial learning to prevent ambigious features. (Intuitively why? I beleive it’s because they’re the ones which are similar across differnt domains. That would be expected right.) | . . Adversarial Domain Adaptation with Domain Mixup . (https://arxiv.org/abs/1912.01805) . tl;dr: Summary of the main idea. . Overall impression . Describe the overall impression of the paper. . Key ideas . Summaries of the key ideas | . Technical details . Summary of technical details | . Notes . Questions and notes on how to improve/revise the current work   | How this work would apply to improve your current research or new research directions | . – . Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach . (https://arxiv.org/abs/1810.11547) . tl;dr: Use information theoretic concepts to do a domain adaptation from a single source to multiple target domains with the unknown labels in the target domain. By posing the problem as an unsupervised domain adaptation which tries to jointly maximize the mutual information between domain labels and the private (domain-specific) features and minimize the mutual information between the domain labels and the shared (domain-invariant) features, the problem is posed as an encoder-decoder setup to overall predict the domain labels, the unknown class labels. Tested on standard domain adaptation datasets showing that they beat the SOTA.   . Overall impression . Describe the overall impression of the paper. . Key ideas . The problem focuses on unsupervised domain adaptation from a single source to multiple targets. | This work tackles the problem of multiple target domains with no labels and a single source domain with labels. The aim is to find an information-theoretic approach to do domain adaptation such that the model finds a shared domain while also accounting for the private domain-specific features. | . Technical details . Posing the problem as a mutual information approach you break down the different information bits you want to optimize accordingly. | I(x;z)I(x;z)I(x;z) - mutual information between the input and the latent space. Encourages the latent space both the z=[zp,zs]z = [z_p, z_s]z=[zp​,zs​] to improve the value | I(y;zs)I(y;z_s)I(y;zs​) - mutual information which encourages output | Overall the problem is posed as an encoder-decoder architecture. Outputs being the reconstruction x^ hat xx^, domain d and the label classification c. | . Notes . They say it is used to tackle a single known source and multiple unknown target domains. Can this be adapted to multiple sources with a known target? How would it need to be adapted? | Lots of information theory math behind why they are doing their setup but overall it seems like a simple enough encoder-decoder architecture setup in an information-theoretic framework. | I liked the fact that they explicitly try to say that there are two separate aspects to a given domain adaptation problem, i.e. the private and the shared domain information and a given network is forced to learn both of them. | . . tl;dr: Summary of the main idea. . Overall impression . Describe the overall impression of the paper. . Key ideas . Summaries of the key ideas | . Technical details . Summary of technical details | . Notes . Questions and notes on how to improve/revise the current work   | How this work would apply to improve your current research or new research directions | .",
            "url": "https://hawkeyedesi.github.io/learningblog/2020/02/17/multisource_distilling_DA.html",
            "relUrl": "/2020/02/17/multisource_distilling_DA.html",
            "date": " • Feb 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Unsupervised_domain_adaptation",
            "content": "Unsupervised Domain adapatation . Most commonly compared against: - . [1702.05464] Adversarial Discriminative Domain Adaptation . [1607.03516] Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation . [1809.02176] Multi-Adversarial Domain Adaptation . Latest Papers from AAAI Conference: . Multi-source Distilling Domain Adaptation . Domain Generalization Using a Mixture of Multiple Latent Domains . Adversarial Domain Adaptation with Domain Mixup . [2001.01046] Adversarial-Learned Loss for Domain Adaptation . Other Papers: . Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach . Reviewer Suggested Papers: . [1801.07593] Mitigating Unwanted Biases with Adversarial Learning This paper attempts to define the different types of biases that can occur and then tries to reduce it using adverserial techinques. The work is shown on both text and tabular data. Results show good outputs without dependence on the bias variable. . High-performance medicine: the convergence of human and artificial intelligence | Nature Medicine . A nice review paper on the overall state of ai in medicine. Nothing in particular related to domain adaptation mentioned although it is a review paper I am sure one of teh many papers cited would have done some bit of it. . International evaluation of an AI system for breast cancer screening | Nature . [1809.07294] Generative Adversarial Network in Medical Imaging: A Review A nice review of gans again not really specifically on domain adapatation. . IJCAI 2018: Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss . Idea is to do a cross domain segmentation using ct and mris. The network is posed as a domain adapatation problem with a adverserarial part. Idea here is to reduce the variances in the feature space between the source and the target dataset since the final task is the same, i.e. whole heart segemntation. This is acheived by having a seperate module trained for the target feature generator part (they call it DAM (domain adapatation module)) alond with a domain critic module (DCM) which uses the wasserstein distance to quanitfy the feature space differences between the MRI and the CT. . only feature space alignment with a few layers. only works on the same modality of the body parts does not or will not probabaly work if the source and the target datasets are acquired differently. . [Kamnitsas et al., 2017]. . Datasets: . Multimodality Whole Heart Segmentation Benchmark needs to be done as well. .",
            "url": "https://hawkeyedesi.github.io/learningblog/2020/02/12/unsupervised_domain_adaptation.html",
            "relUrl": "/2020/02/12/unsupervised_domain_adaptation.html",
            "date": " • Feb 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Weightagnostic_nn",
            "content": "[REVIEW] Weight Agnostic Neural Networks . Resources . Arxiv Paper | Blogpost | Slides | Blog Github | Tutorial on MDL | . Note: I am basing everything in these notes from the excellent blog post. I haven’t gone through the paper yet. . tl;dr: The paper introduces a method of designing neural network architectures that are not dependent on the node weights. Rather the trick here is to let the network evolve its architecture such that the architecture itself has the capacity required to solve the required tasks. The authors draw analogies with precocial animals such as snakes and iguanas wherein they are wired to run away from the prey. . Overall impression . I think its a very cool concept in its nascent stages they have shown how without the burden of weight training, they employed a topology-based algorithm (NEAT) to find and grow the topology. . Key ideas . Quoting from the blog: . While the aforementioned works focus on the information capacity required to represent the weights of predefined network architecture, in this work we focus on finding minimal architectures that can represent solutions to various tasks. . By making the network be independent of the weights or the tuning and solely depending on the architecture the work can use other algorithms besides gradient descent and backprop such as NEAT to solve the architecture . | By weight sharing across the entire network with the weighting treated as a random sample from a uniform random distribution. . | Introducing more than the common (e.g. linear, sigmoid, ReLU) and more exotic (Gaussian, sinusoid, step), the network is learning to include more of these connections. . | By replacing the grunt work of the weights as filters, this work is now forcing the network connections to provide the right weighting or inductive biases for that particular problem. . | Further, instead of fixing the structure of the network and then biasing the weights of the network to solve a specific problem, you are instead relying on the network architecture structure. . | By introducing nodes with a lot more sets of options such as inv, sin, cost gaussian, bias, etc (See paper and the code for details) you are replicating the functions a weight-based filter would do. That is a pretty cool concept. . | Unlike NAS or other operations wherein you have to run the network every single time to compute a metric (for eg. accuracy) to see the performance, this proposed method instead just goes with random permutations of the network (I am assuming that’s what the random trial paper does or some flavor of it). . | . Technical details . Not merely any weight agnostic neural network but one that can be described using a minimum descriptor length. . | Significance of minimum description length models when choosing the weight agnostic NN: Using MDL inference, you are ensuring that the model that is being selected is the simplest one while still ensuring that it works just as well as the more complex network architectures. . | . Notes . Inductive Biases in Machine learning are a set of assumptions that the model learns to make during training to predict the unseen target data accurately. This website on inductive bias has a nice list of biases that are present for every task. . | What is Minimum Description Length: MDL is an inference method that provides a generic solution to the model inference problem. The definition states that when you have different models that perform just as well, using MDL for model selection will ensure that you pick the simplest model that still performs as well as the more complex models. . | Questions and notes on how to improve/revise the current work How would it work for more complicated semi-supervised and supervised problems? | Are there more creative and interesting node functions to pick from that will help improve this? | What happens if you start allowing more interesting connections such as skip connections in the feed-forward directions. Can you even do that and can it be optimized using the NEAT algorithm?   Things I don’t understand  . | . | What are the connection cost technique and I am not sure how it relates to the NEAT algorithm for model growing? . | Further, how does dominance relations work to rank the networks based on mean overall performance, the weight values (how does it perform for the fixed selected weight. The paper chose a range between [-2,2]), max performance of the single best value and the number of connections in the network. . | Does the domain relation ranking also consider the MDL inference internally and if so how? I still need to read those topics. | .",
            "url": "https://hawkeyedesi.github.io/learningblog/2020/01/30/weightagnostic_nn.html",
            "relUrl": "/2020/01/30/weightagnostic_nn.html",
            "date": " • Jan 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Self_attention_iclr2020",
            "content": "[REVIEW] On the Relationship between Self Attention and Convolutional Layers . Background: . This paper banks on the concept of transformers and self attention both of which are foriegn to me. So i start out doing a brief review on what the two concepts are and then move onto what this paper does and how it uses them. . Review of the Illustrated Transformer Blog Post. To start off with you need to first get an understanding of the attention mechanism in the Seq2Seq models starting here . Key Ideas from the two blog posts on attention and transformers: . Attention Mechanism: . The attention mechanism is the same concept as the attention that we think of in computer vision as heatmaps. In there the common practise is to use a grad-cam to backpropogate and weight the regions of the image which correspond to that class output. The same concept applies here as well. You use the idea of weighting the hidden layers of the encoder for every step of the decoder to highlight the importance. This is used along with the hidden layer of the decoder to predict the output of every step of the decoder. . | One key difference in using attention networks versus classic seq-to-seq networks is the amount of information that gets passed onto the decoder. In classic case only the final Hidden State is passed to the decoder. Here all the hidden states are passed. . | At each time step of the decoder, each of the hidden layers from the encoder is then weighted according to the scoring function (normalized) to give higher importance or attention to the relavent hidden state. . | Further in transformers there is an attention mechanism in the encoder part as well . | Note that the scoring is calculated at each time step of the decoder. . | . This excellent illustration from Jay Alammar shows a gist of the seq2seq with attention. Plese refer to the original blogpost for further details: . Your browser does not support the video tag. The transformer: . The transform is a different beast from what we have seen with the standard CNNs. It’s also based on the attention mechanism but in a highly structured way. Compared to the earlier attention mechanism in the seq2seq work with attention, this one has three different parts (Query, Key, Value) to the self-attention mechanism. . | The entire transformer network idea is to increase the number of self-attention heads and initialize them randomly so that you have enough different WQ,WK,WVW_Q, W_K, W_VWQ​,WK​,WV​ matrices trained to ensure that the words are propoerly attended to. . | Further the decoder part of the network also has a similar attention mechanism to attend to the enconding output vectors. Unlike the original seq2seq with attention, here the network looks at the key and the value embeddings produced from the output of the final layer of the encoder. The weighting provided by the Query term of the decoder which only looks at the earlier positions of the output sequence is used to weight the attention for each decoder vector. . | Two additional conceps of residual connections and positional encodings for the input vector embeddings are used to further enchance the network. Residual connections function similar to the residual blocks that are used in Resnets and DenseNets of computer vision, i.e. they reduce the problem of vanishing gradient which will affect such huge networks like the transformers. The positional encoding is an intersting idea. They way I think of it is to liken it to a method which helps spatially encoder the positioning. The original paper broke the two halves of the 512 vector embedding into sine and cosine based embedding vectors and then concatenated them. . | . Additional Resource: https://www.youtube.com/watch?v=rBCqOTEfxvg http://nlp.seas.harvard.edu/2018/04/03/attention.html - A full step by step implementation of the paper with comments. . . Arxiv paper: ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS . Blog post: http://jbcordonnier.com/posts/attention-cnn/ . tl;dr: Self attention can express convolutional layer and the filters can be learnt in practise. The authors prove a theorem showing that with enough attention head, the self-attention can generalize the convolutional layer. I still haven’t read the paper in detail but their blog post gave me enough of an idea to understand what the paper is doing . Overall impression . Intersting take on correlating the attention head and cnns for images. The CNNs were originally designed to have limited to small fixed kernels which translated across the image to avoid having the whole image have differnt kernel weights. Now with the attention, it can attend to the whole image once again by posing it as an attention query problem. . Key ideas . Having multi head attention can generalize a CNN. Paper proves this theorem. | With multiple heads ensures that all areas of the imgage are attended to. | With positional encoding the problem of attention mechanism having equivariance is allieviated. | Results on CFAIR show comparable performance to standard CNNs | . Technical details . Attention mechanism translated from 1D to images: Basically instead of having token length embeddings for every word every single pixel i,ji,ji,j is assigned a key - value score. There is a bit of notation abuse in the paper wherin every ApA_pAp​ and XpX_pXp​ denotes a 2D index corresponding to the (x,y)(x,y)(x,y) pixel position. | . Notes . Questions and notes on how to improve/revise the current work Authors propose conditioning the receptive field on the input pixels | Can this attention mechanism improve explainability. Can we better visualize what each region of the multiple heads are attending to for a given pixel or a region of pixels? | Using transformers causes an explosion of paramters. And the main factor which is limiting it is the assumption that you take every pixel i,ji,ji,j and attend to it. One simplification is to replicate what the embedding vector does in NLP. The same words would have the same embeddings. Maybe a simplification would be to have key-value pairs based on pixel intensity? Will the work out? | . | . I skimmed the proof because I really don’t have any experience understanding what it is doing. .",
            "url": "https://hawkeyedesi.github.io/learningblog/2020/01/27/self_attention_iclr2020.html",
            "relUrl": "/2020/01/27/self_attention_iclr2020.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Blackbox_combinatorial_solver_iclr2020",
            "content": "[REVIEW] Differentiation of Blackbox Combinatorial Solvers . Arxiv Paper: Differentiation of Blackbox Combinatorial Solvers . tl;dr: This paper presents for the first time a method for combining deep learning as a rich feature generator along with a combinatorial optimizer. The innovative thing they show here is that by approximating the piecewise continuous outputs from a combinatorial optimizer to make it continuous we can backpropagate the gradients to also learn the feature representations of the deep learner.   Also disclaimer: This is all that I understood from the blog post and haven’t read the paper in detail. . Overall impression . I think it is a cool idea that you can present to the combinatorial optimization a set of learnable features for the problem using such a rich feature representer like deep learning while also solving complex combinatorial optimization. . Key ideas . Realize that by posing the combinatorial optimizer as one of the intermediate layers of the deep learner you can solve the problem along with optimizing the weights of the deep learner to improve the feature representation. | By ensuring that the output of the combinatorial optimizer can be made continuous by maintaining the same minima and maxima, you can end up passing the gradients through the combinatorial optimizer to update the weights of the neural network. | Different from the other papers is the fact that does a smarter piecewise approximation.   | They take existing combinatorial solvers and use it as a black box “Layer” for deep learning. | . Technical details . I don’t fully understand it. | . Notes . Questions and notes on how to improve/revise the current work   . | How this work would apply to improve your current research or new research directions   - What would happen if you had another 1D deep learning interpolator to improve the approximation of the piece-wise continuous function? How much dependent is the learner as a function of the optimizer? . | . Extra resources: . https://towardsdatascience.com/the-fusion-of-deep-learning-and-combinatorics-4d0112a74fa7 | Extra points to note | .",
            "url": "https://hawkeyedesi.github.io/learningblog/2020/01/27/blackbox_combinatorial_solver_iclr2020.html",
            "relUrl": "/2020/01/27/blackbox_combinatorial_solver_iclr2020.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hawkeyedesi.github.io/learningblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hawkeyedesi.github.io/learningblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}